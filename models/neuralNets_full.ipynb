{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "\n",
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>459</th>\n",
       "      <th>460</th>\n",
       "      <th>461</th>\n",
       "      <th>462</th>\n",
       "      <th>463</th>\n",
       "      <th>464</th>\n",
       "      <th>465</th>\n",
       "      <th>466</th>\n",
       "      <th>467</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.156041</td>\n",
       "      <td>0.034724</td>\n",
       "      <td>0.071444</td>\n",
       "      <td>0.078117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043656</td>\n",
       "      <td>0.147673</td>\n",
       "      <td>0.290654</td>\n",
       "      <td>0.227140</td>\n",
       "      <td>0.271585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063550</td>\n",
       "      <td>0.104668</td>\n",
       "      <td>0.057380</td>\n",
       "      <td>0.062866</td>\n",
       "      <td>0.189442</td>\n",
       "      <td>0.170856</td>\n",
       "      <td>0.159821</td>\n",
       "      <td>0.323322</td>\n",
       "      <td>0.348375</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.099105</td>\n",
       "      <td>0.033467</td>\n",
       "      <td>0.048210</td>\n",
       "      <td>0.096153</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050459</td>\n",
       "      <td>0.181863</td>\n",
       "      <td>0.332776</td>\n",
       "      <td>0.276899</td>\n",
       "      <td>0.323902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061139</td>\n",
       "      <td>0.101462</td>\n",
       "      <td>0.051161</td>\n",
       "      <td>0.050882</td>\n",
       "      <td>0.230257</td>\n",
       "      <td>0.214284</td>\n",
       "      <td>0.201468</td>\n",
       "      <td>0.353529</td>\n",
       "      <td>0.377521</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.160259</td>\n",
       "      <td>0.033538</td>\n",
       "      <td>0.105772</td>\n",
       "      <td>0.086060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046421</td>\n",
       "      <td>0.181095</td>\n",
       "      <td>0.224147</td>\n",
       "      <td>0.273403</td>\n",
       "      <td>0.312137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071666</td>\n",
       "      <td>0.147298</td>\n",
       "      <td>0.067166</td>\n",
       "      <td>0.087085</td>\n",
       "      <td>0.274896</td>\n",
       "      <td>0.251641</td>\n",
       "      <td>0.229446</td>\n",
       "      <td>0.382837</td>\n",
       "      <td>0.408696</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.139705</td>\n",
       "      <td>0.035719</td>\n",
       "      <td>0.060722</td>\n",
       "      <td>0.098022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051891</td>\n",
       "      <td>0.187796</td>\n",
       "      <td>0.330197</td>\n",
       "      <td>0.296981</td>\n",
       "      <td>0.350095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063510</td>\n",
       "      <td>0.106100</td>\n",
       "      <td>0.055346</td>\n",
       "      <td>0.057549</td>\n",
       "      <td>0.232088</td>\n",
       "      <td>0.215398</td>\n",
       "      <td>0.203230</td>\n",
       "      <td>0.354787</td>\n",
       "      <td>0.383226</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.144930</td>\n",
       "      <td>0.030739</td>\n",
       "      <td>0.065995</td>\n",
       "      <td>0.082237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041594</td>\n",
       "      <td>0.145923</td>\n",
       "      <td>0.312308</td>\n",
       "      <td>0.210267</td>\n",
       "      <td>0.247076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035856</td>\n",
       "      <td>0.051338</td>\n",
       "      <td>0.042271</td>\n",
       "      <td>0.047047</td>\n",
       "      <td>0.154600</td>\n",
       "      <td>0.146428</td>\n",
       "      <td>0.139789</td>\n",
       "      <td>0.194182</td>\n",
       "      <td>0.204608</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 469 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3    4         5         6         7  \\\n",
       "0  0.156041  0.034724  0.071444  0.078117  0.0  0.043656  0.147673  0.290654   \n",
       "1  0.099105  0.033467  0.048210  0.096153  0.0  0.050459  0.181863  0.332776   \n",
       "2  0.160259  0.033538  0.105772  0.086060  0.0  0.046421  0.181095  0.224147   \n",
       "3  0.139705  0.035719  0.060722  0.098022  0.0  0.051891  0.187796  0.330197   \n",
       "4  0.144930  0.030739  0.065995  0.082237  0.0  0.041594  0.145923  0.312308   \n",
       "\n",
       "          8         9  ...       459       460       461       462       463  \\\n",
       "0  0.227140  0.271585  ...  0.063550  0.104668  0.057380  0.062866  0.189442   \n",
       "1  0.276899  0.323902  ...  0.061139  0.101462  0.051161  0.050882  0.230257   \n",
       "2  0.273403  0.312137  ...  0.071666  0.147298  0.067166  0.087085  0.274896   \n",
       "3  0.296981  0.350095  ...  0.063510  0.106100  0.055346  0.057549  0.232088   \n",
       "4  0.210267  0.247076  ...  0.035856  0.051338  0.042271  0.047047  0.154600   \n",
       "\n",
       "        464       465       466       467  label  \n",
       "0  0.170856  0.159821  0.323322  0.348375      0  \n",
       "1  0.214284  0.201468  0.353529  0.377521      0  \n",
       "2  0.251641  0.229446  0.382837  0.408696      0  \n",
       "3  0.215398  0.203230  0.354787  0.383226      0  \n",
       "4  0.146428  0.139789  0.194182  0.204608      0  \n",
       "\n",
       "[5 rows x 469 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data.csv')\n",
    "data.drop('Index', axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split variables from label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"./logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 468)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 24)                11256     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 150       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,406\n",
      "Trainable params: 11,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape = 468)\n",
    "\n",
    "z = layers.Dense(24, activation=\"relu\")(inputs)\n",
    "outputs = layers.Dense(6, activation=\"softmax\")(z)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "610/610 [==============================] - 2s 2ms/step - loss: 1.7446 - accuracy: 0.2643 - val_loss: 1.7314 - val_accuracy: 0.2625\n",
      "Epoch 2/50\n",
      "610/610 [==============================] - 1s 2ms/step - loss: 1.7170 - accuracy: 0.2693 - val_loss: 1.6994 - val_accuracy: 0.2912\n",
      "Epoch 3/50\n",
      "610/610 [==============================] - 1s 2ms/step - loss: 1.6796 - accuracy: 0.2923 - val_loss: 1.6647 - val_accuracy: 0.3572\n",
      "Epoch 4/50\n",
      "610/610 [==============================] - 1s 2ms/step - loss: 1.6422 - accuracy: 0.3266 - val_loss: 1.6159 - val_accuracy: 0.3443\n",
      "Epoch 5/50\n",
      "610/610 [==============================] - 1s 2ms/step - loss: 1.6058 - accuracy: 0.3516 - val_loss: 1.5804 - val_accuracy: 0.3375\n",
      "Epoch 6/50\n",
      "610/610 [==============================] - 1s 2ms/step - loss: 1.5745 - accuracy: 0.3704 - val_loss: 1.5636 - val_accuracy: 0.3412\n",
      "Epoch 7/50\n",
      "610/610 [==============================] - 1s 2ms/step - loss: 1.5510 - accuracy: 0.3827 - val_loss: 1.5238 - val_accuracy: 0.4063\n",
      "Epoch 8/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.5312 - accuracy: 0.3941 - val_loss: 1.5375 - val_accuracy: 0.3508\n",
      "Epoch 9/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.5141 - accuracy: 0.4042 - val_loss: 1.5048 - val_accuracy: 0.3897\n",
      "Epoch 10/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.5050 - accuracy: 0.4063 - val_loss: 1.4916 - val_accuracy: 0.4152\n",
      "Epoch 11/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4954 - accuracy: 0.4113 - val_loss: 1.4959 - val_accuracy: 0.3842\n",
      "Epoch 12/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4845 - accuracy: 0.4155 - val_loss: 1.4908 - val_accuracy: 0.4209\n",
      "Epoch 13/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4775 - accuracy: 0.4201 - val_loss: 1.4991 - val_accuracy: 0.3757\n",
      "Epoch 14/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4751 - accuracy: 0.4194 - val_loss: 1.4593 - val_accuracy: 0.4277\n",
      "Epoch 15/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4700 - accuracy: 0.4243 - val_loss: 1.4844 - val_accuracy: 0.3925\n",
      "Epoch 16/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4649 - accuracy: 0.4218 - val_loss: 1.4524 - val_accuracy: 0.4322\n",
      "Epoch 17/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4640 - accuracy: 0.4210 - val_loss: 1.4479 - val_accuracy: 0.4286\n",
      "Epoch 18/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4596 - accuracy: 0.4235 - val_loss: 1.4543 - val_accuracy: 0.4165\n",
      "Epoch 19/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4561 - accuracy: 0.4250 - val_loss: 1.4450 - val_accuracy: 0.4245\n",
      "Epoch 20/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4601 - accuracy: 0.4232 - val_loss: 1.4403 - val_accuracy: 0.4331\n",
      "Epoch 21/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4545 - accuracy: 0.4252 - val_loss: 1.4579 - val_accuracy: 0.4189\n",
      "Epoch 22/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4510 - accuracy: 0.4260 - val_loss: 1.4459 - val_accuracy: 0.4294\n",
      "Epoch 23/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4525 - accuracy: 0.4247 - val_loss: 1.4375 - val_accuracy: 0.4305\n",
      "Epoch 24/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4481 - accuracy: 0.4280 - val_loss: 1.4481 - val_accuracy: 0.4138\n",
      "Epoch 25/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4460 - accuracy: 0.4292 - val_loss: 1.4361 - val_accuracy: 0.4323\n",
      "Epoch 26/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4427 - accuracy: 0.4286 - val_loss: 1.4587 - val_accuracy: 0.4248\n",
      "Epoch 27/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4452 - accuracy: 0.4282 - val_loss: 1.4588 - val_accuracy: 0.4203\n",
      "Epoch 28/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4423 - accuracy: 0.4290 - val_loss: 1.4543 - val_accuracy: 0.4255\n",
      "Epoch 29/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4408 - accuracy: 0.4291 - val_loss: 1.4298 - val_accuracy: 0.4345\n",
      "Epoch 30/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4397 - accuracy: 0.4313 - val_loss: 1.4307 - val_accuracy: 0.4329\n",
      "Epoch 31/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4366 - accuracy: 0.4358 - val_loss: 1.4659 - val_accuracy: 0.3942\n",
      "Epoch 32/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4357 - accuracy: 0.4338 - val_loss: 1.4384 - val_accuracy: 0.4317\n",
      "Epoch 33/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4365 - accuracy: 0.4339 - val_loss: 1.4286 - val_accuracy: 0.4220\n",
      "Epoch 34/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4268 - accuracy: 0.4411 - val_loss: 1.4332 - val_accuracy: 0.4312\n",
      "Epoch 35/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4269 - accuracy: 0.4373 - val_loss: 1.4211 - val_accuracy: 0.4425\n",
      "Epoch 36/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4239 - accuracy: 0.4423 - val_loss: 1.4488 - val_accuracy: 0.4272\n",
      "Epoch 37/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4226 - accuracy: 0.4403 - val_loss: 1.4103 - val_accuracy: 0.4431\n",
      "Epoch 38/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4227 - accuracy: 0.4421 - val_loss: 1.4272 - val_accuracy: 0.4431\n",
      "Epoch 39/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4163 - accuracy: 0.4456 - val_loss: 1.4148 - val_accuracy: 0.4309\n",
      "Epoch 40/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4177 - accuracy: 0.4439 - val_loss: 1.4635 - val_accuracy: 0.4278\n",
      "Epoch 41/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4162 - accuracy: 0.4461 - val_loss: 1.4070 - val_accuracy: 0.4397\n",
      "Epoch 42/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4137 - accuracy: 0.4459 - val_loss: 1.4312 - val_accuracy: 0.4183\n",
      "Epoch 43/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4103 - accuracy: 0.4444 - val_loss: 1.4126 - val_accuracy: 0.4452\n",
      "Epoch 44/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4100 - accuracy: 0.4449 - val_loss: 1.4319 - val_accuracy: 0.4223\n",
      "Epoch 45/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4122 - accuracy: 0.4423 - val_loss: 1.4147 - val_accuracy: 0.4494\n",
      "Epoch 46/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4060 - accuracy: 0.4449 - val_loss: 1.4012 - val_accuracy: 0.4409\n",
      "Epoch 47/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4057 - accuracy: 0.4476 - val_loss: 1.3969 - val_accuracy: 0.4531\n",
      "Epoch 48/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4061 - accuracy: 0.4446 - val_loss: 1.4162 - val_accuracy: 0.4471\n",
      "Epoch 49/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.4036 - accuracy: 0.4472 - val_loss: 1.4030 - val_accuracy: 0.4278\n",
      "Epoch 50/50\n",
      "610/610 [==============================] - 1s 1ms/step - loss: 1.3993 - accuracy: 0.4492 - val_loss: 1.3949 - val_accuracy: 0.4515\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "callback = keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, # input\n",
    "    y_train, # output\n",
    "    batch_size=batch_size,\n",
    "    verbose=1, \n",
    "    epochs=50,\n",
    "    shuffle=True,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=[callback, tensorboard_callback],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "predictions = keras.backend.argmax(\n",
    "    predictions,\n",
    "    axis=-1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44753846153846155"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(n_components=0.99)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(.99)\n",
    "pca.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pca.transform(x_train)\n",
    "test = pca.transform(x_test)\n",
    "val = pca.transform(x_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19498, 7)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 468)]             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 24)                11256     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 150       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,406\n",
      "Trainable params: 11,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.3940 - accuracy: 0.4503 - val_loss: 1.4041 - val_accuracy: 0.4509\n",
      "Epoch 2/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.3922 - accuracy: 0.4521 - val_loss: 1.3929 - val_accuracy: 0.4554\n",
      "Epoch 3/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.3929 - accuracy: 0.4497 - val_loss: 1.3913 - val_accuracy: 0.4543\n",
      "Epoch 4/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.3928 - accuracy: 0.4510 - val_loss: 1.4056 - val_accuracy: 0.4328\n",
      "Epoch 5/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3925 - accuracy: 0.4506 - val_loss: 1.3894 - val_accuracy: 0.4549\n",
      "Epoch 6/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3903 - accuracy: 0.4505 - val_loss: 1.3882 - val_accuracy: 0.4552\n",
      "Epoch 7/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3908 - accuracy: 0.4513 - val_loss: 1.3857 - val_accuracy: 0.4554\n",
      "Epoch 8/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.3896 - accuracy: 0.4511 - val_loss: 1.3960 - val_accuracy: 0.4543\n",
      "Epoch 9/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.3905 - accuracy: 0.4494 - val_loss: 1.3952 - val_accuracy: 0.4526\n",
      "Epoch 10/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3895 - accuracy: 0.4518 - val_loss: 1.3938 - val_accuracy: 0.4446\n",
      "Epoch 11/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3907 - accuracy: 0.4499 - val_loss: 1.3900 - val_accuracy: 0.4472\n",
      "Epoch 12/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3903 - accuracy: 0.4495 - val_loss: 1.3870 - val_accuracy: 0.4537\n",
      "Epoch 13/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3864 - accuracy: 0.4540 - val_loss: 1.3890 - val_accuracy: 0.4425\n",
      "Epoch 14/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3877 - accuracy: 0.4489 - val_loss: 1.3811 - val_accuracy: 0.4563\n",
      "Epoch 15/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3895 - accuracy: 0.4513 - val_loss: 1.3901 - val_accuracy: 0.4392\n",
      "Epoch 16/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3863 - accuracy: 0.4477 - val_loss: 1.3875 - val_accuracy: 0.4448\n",
      "Epoch 17/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3852 - accuracy: 0.4521 - val_loss: 1.3807 - val_accuracy: 0.4523\n",
      "Epoch 18/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3828 - accuracy: 0.4519 - val_loss: 1.3780 - val_accuracy: 0.4514\n",
      "Epoch 19/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3841 - accuracy: 0.4507 - val_loss: 1.3939 - val_accuracy: 0.4535\n",
      "Epoch 20/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3814 - accuracy: 0.4512 - val_loss: 1.3812 - val_accuracy: 0.4508\n",
      "Epoch 21/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3812 - accuracy: 0.4515 - val_loss: 1.3779 - val_accuracy: 0.4572\n",
      "Epoch 22/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3805 - accuracy: 0.4534 - val_loss: 1.3791 - val_accuracy: 0.4586\n",
      "Epoch 23/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3789 - accuracy: 0.4498 - val_loss: 1.3787 - val_accuracy: 0.4543\n",
      "Epoch 24/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3801 - accuracy: 0.4503 - val_loss: 1.3843 - val_accuracy: 0.4492\n",
      "Epoch 25/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3781 - accuracy: 0.4532 - val_loss: 1.3835 - val_accuracy: 0.4489\n",
      "Epoch 26/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3791 - accuracy: 0.4540 - val_loss: 1.3729 - val_accuracy: 0.4546\n",
      "Epoch 27/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3775 - accuracy: 0.4539 - val_loss: 1.3833 - val_accuracy: 0.4382\n",
      "Epoch 28/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3756 - accuracy: 0.4528 - val_loss: 1.3709 - val_accuracy: 0.4560\n",
      "Epoch 29/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3735 - accuracy: 0.4543 - val_loss: 1.3856 - val_accuracy: 0.4622\n",
      "Epoch 30/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3787 - accuracy: 0.4527 - val_loss: 1.3702 - val_accuracy: 0.4535\n",
      "Epoch 31/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3731 - accuracy: 0.4527 - val_loss: 1.3905 - val_accuracy: 0.4589\n",
      "Epoch 32/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3745 - accuracy: 0.4507 - val_loss: 1.3698 - val_accuracy: 0.4542\n",
      "Epoch 33/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3704 - accuracy: 0.4547 - val_loss: 1.3773 - val_accuracy: 0.4443\n",
      "Epoch 34/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3702 - accuracy: 0.4548 - val_loss: 1.3685 - val_accuracy: 0.4555\n",
      "Epoch 35/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3680 - accuracy: 0.4548 - val_loss: 1.3686 - val_accuracy: 0.4603\n",
      "Epoch 36/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3715 - accuracy: 0.4543 - val_loss: 1.3707 - val_accuracy: 0.4649\n",
      "Epoch 37/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.3693 - accuracy: 0.4538 - val_loss: 1.3622 - val_accuracy: 0.4635\n",
      "Epoch 38/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3666 - accuracy: 0.4554 - val_loss: 1.3890 - val_accuracy: 0.4422\n",
      "Epoch 39/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3659 - accuracy: 0.4562 - val_loss: 1.3673 - val_accuracy: 0.4557\n",
      "Epoch 40/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3679 - accuracy: 0.4557 - val_loss: 1.3729 - val_accuracy: 0.4625\n",
      "Epoch 41/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3620 - accuracy: 0.4572 - val_loss: 1.3748 - val_accuracy: 0.4512\n",
      "Epoch 42/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3676 - accuracy: 0.4550 - val_loss: 1.3564 - val_accuracy: 0.4665\n",
      "Epoch 43/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3614 - accuracy: 0.4582 - val_loss: 1.3756 - val_accuracy: 0.4468\n",
      "Epoch 44/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3608 - accuracy: 0.4573 - val_loss: 1.3583 - val_accuracy: 0.4580\n",
      "Epoch 45/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3576 - accuracy: 0.4594 - val_loss: 1.3561 - val_accuracy: 0.4588\n",
      "Epoch 46/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3565 - accuracy: 0.4600 - val_loss: 1.3548 - val_accuracy: 0.4698\n",
      "Epoch 47/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3593 - accuracy: 0.4582 - val_loss: 1.3531 - val_accuracy: 0.4622\n",
      "Epoch 48/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3530 - accuracy: 0.4595 - val_loss: 1.3559 - val_accuracy: 0.4609\n",
      "Epoch 49/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3555 - accuracy: 0.4618 - val_loss: 1.3775 - val_accuracy: 0.4458\n",
      "Epoch 50/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3545 - accuracy: 0.4615 - val_loss: 1.3617 - val_accuracy: 0.4618\n",
      "Epoch 51/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3512 - accuracy: 0.4606 - val_loss: 1.3532 - val_accuracy: 0.4617\n",
      "Epoch 52/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3465 - accuracy: 0.4646 - val_loss: 1.3495 - val_accuracy: 0.4615\n",
      "Epoch 53/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3493 - accuracy: 0.4624 - val_loss: 1.3470 - val_accuracy: 0.4662\n",
      "Epoch 54/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.3465 - accuracy: 0.4659 - val_loss: 1.3645 - val_accuracy: 0.4542\n",
      "Epoch 55/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3464 - accuracy: 0.4634 - val_loss: 1.3413 - val_accuracy: 0.4708\n",
      "Epoch 56/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3474 - accuracy: 0.4659 - val_loss: 1.3389 - val_accuracy: 0.4597\n",
      "Epoch 57/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3418 - accuracy: 0.4656 - val_loss: 1.3439 - val_accuracy: 0.4717\n",
      "Epoch 58/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3428 - accuracy: 0.4701 - val_loss: 1.3575 - val_accuracy: 0.4505\n",
      "Epoch 59/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.3383 - accuracy: 0.4683 - val_loss: 1.3366 - val_accuracy: 0.4743\n",
      "Epoch 60/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.3392 - accuracy: 0.4696 - val_loss: 1.3379 - val_accuracy: 0.4745\n",
      "Epoch 61/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3341 - accuracy: 0.4712 - val_loss: 1.3834 - val_accuracy: 0.4435\n",
      "Epoch 62/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3390 - accuracy: 0.4716 - val_loss: 1.3431 - val_accuracy: 0.4585\n",
      "Epoch 63/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.3362 - accuracy: 0.4723 - val_loss: 1.3306 - val_accuracy: 0.4726\n",
      "Epoch 64/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3353 - accuracy: 0.4717 - val_loss: 1.3379 - val_accuracy: 0.4791\n",
      "Epoch 65/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3319 - accuracy: 0.4703 - val_loss: 1.3255 - val_accuracy: 0.4771\n",
      "Epoch 66/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3318 - accuracy: 0.4722 - val_loss: 1.3301 - val_accuracy: 0.4718\n",
      "Epoch 67/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3297 - accuracy: 0.4750 - val_loss: 1.3384 - val_accuracy: 0.4749\n",
      "Epoch 68/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3316 - accuracy: 0.4710 - val_loss: 1.3435 - val_accuracy: 0.4620\n",
      "Epoch 69/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3288 - accuracy: 0.4725 - val_loss: 1.3396 - val_accuracy: 0.4552\n",
      "Epoch 70/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3310 - accuracy: 0.4756 - val_loss: 1.3662 - val_accuracy: 0.4597\n",
      "Epoch 71/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3288 - accuracy: 0.4713 - val_loss: 1.3302 - val_accuracy: 0.4754\n",
      "Epoch 72/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3247 - accuracy: 0.4755 - val_loss: 1.3389 - val_accuracy: 0.4706\n",
      "Epoch 73/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3266 - accuracy: 0.4751 - val_loss: 1.3206 - val_accuracy: 0.4786\n",
      "Epoch 74/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3226 - accuracy: 0.4759 - val_loss: 1.3550 - val_accuracy: 0.4626\n",
      "Epoch 75/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3233 - accuracy: 0.4757 - val_loss: 1.3197 - val_accuracy: 0.4738\n",
      "Epoch 76/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3251 - accuracy: 0.4751 - val_loss: 1.3166 - val_accuracy: 0.4772\n",
      "Epoch 77/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3238 - accuracy: 0.4743 - val_loss: 1.3147 - val_accuracy: 0.4806\n",
      "Epoch 78/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3212 - accuracy: 0.4771 - val_loss: 1.3252 - val_accuracy: 0.4709\n",
      "Epoch 79/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3214 - accuracy: 0.4765 - val_loss: 1.3691 - val_accuracy: 0.4605\n",
      "Epoch 80/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3214 - accuracy: 0.4782 - val_loss: 1.3329 - val_accuracy: 0.4734\n",
      "Epoch 81/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3200 - accuracy: 0.4760 - val_loss: 1.3251 - val_accuracy: 0.4678\n",
      "Epoch 82/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3226 - accuracy: 0.4738 - val_loss: 1.3190 - val_accuracy: 0.4803\n",
      "Epoch 83/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3193 - accuracy: 0.4786 - val_loss: 1.3296 - val_accuracy: 0.4678\n",
      "Epoch 84/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.3185 - accuracy: 0.4765 - val_loss: 1.3140 - val_accuracy: 0.4806\n",
      "Epoch 85/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3168 - accuracy: 0.4783 - val_loss: 1.3163 - val_accuracy: 0.4743\n",
      "Epoch 86/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3141 - accuracy: 0.4797 - val_loss: 1.3150 - val_accuracy: 0.4751\n",
      "Epoch 87/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3159 - accuracy: 0.4797 - val_loss: 1.3214 - val_accuracy: 0.4760\n",
      "Epoch 88/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3168 - accuracy: 0.4769 - val_loss: 1.3140 - val_accuracy: 0.4828\n",
      "Epoch 89/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3151 - accuracy: 0.4794 - val_loss: 1.3192 - val_accuracy: 0.4775\n",
      "Epoch 90/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3141 - accuracy: 0.4804 - val_loss: 1.3161 - val_accuracy: 0.4740\n",
      "Epoch 91/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3171 - accuracy: 0.4788 - val_loss: 1.3078 - val_accuracy: 0.4828\n",
      "Epoch 92/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3149 - accuracy: 0.4807 - val_loss: 1.3136 - val_accuracy: 0.4808\n",
      "Epoch 93/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3120 - accuracy: 0.4782 - val_loss: 1.3176 - val_accuracy: 0.4763\n",
      "Epoch 94/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.3181 - accuracy: 0.4803 - val_loss: 1.3369 - val_accuracy: 0.4617\n",
      "Epoch 95/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.3158 - accuracy: 0.4802 - val_loss: 1.3093 - val_accuracy: 0.4802\n",
      "Epoch 96/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3087 - accuracy: 0.4809 - val_loss: 1.3399 - val_accuracy: 0.4603\n",
      "Epoch 97/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.3112 - accuracy: 0.4800 - val_loss: 1.3606 - val_accuracy: 0.4640\n",
      "Epoch 98/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3131 - accuracy: 0.4798 - val_loss: 1.3047 - val_accuracy: 0.4795\n",
      "Epoch 99/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3097 - accuracy: 0.4832 - val_loss: 1.3099 - val_accuracy: 0.4843\n",
      "Epoch 100/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3103 - accuracy: 0.4799 - val_loss: 1.3060 - val_accuracy: 0.4805\n",
      "Epoch 101/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3116 - accuracy: 0.4816 - val_loss: 1.3184 - val_accuracy: 0.4738\n",
      "Epoch 102/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3158 - accuracy: 0.4769 - val_loss: 1.3784 - val_accuracy: 0.4602\n",
      "Epoch 103/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.3145 - accuracy: 0.4796 - val_loss: 1.3086 - val_accuracy: 0.4815\n",
      "Epoch 104/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3083 - accuracy: 0.4839 - val_loss: 1.3082 - val_accuracy: 0.4788\n",
      "Epoch 105/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.3098 - accuracy: 0.4785 - val_loss: 1.3080 - val_accuracy: 0.4849\n",
      "Epoch 106/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.3101 - accuracy: 0.4824 - val_loss: 1.3129 - val_accuracy: 0.4782\n",
      "Epoch 107/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3130 - accuracy: 0.4807 - val_loss: 1.3059 - val_accuracy: 0.4808\n",
      "Epoch 108/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.3074 - accuracy: 0.4810 - val_loss: 1.3335 - val_accuracy: 0.4706\n",
      "Epoch 109/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3090 - accuracy: 0.4803 - val_loss: 1.3488 - val_accuracy: 0.4594\n",
      "Epoch 110/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3059 - accuracy: 0.4813 - val_loss: 1.3013 - val_accuracy: 0.4862\n",
      "Epoch 111/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3073 - accuracy: 0.4802 - val_loss: 1.3129 - val_accuracy: 0.4791\n",
      "Epoch 112/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3110 - accuracy: 0.4778 - val_loss: 1.3012 - val_accuracy: 0.4837\n",
      "Epoch 113/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3047 - accuracy: 0.4842 - val_loss: 1.3198 - val_accuracy: 0.4705\n",
      "Epoch 114/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3055 - accuracy: 0.4838 - val_loss: 1.3045 - val_accuracy: 0.4788\n",
      "Epoch 115/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3052 - accuracy: 0.4821 - val_loss: 1.3025 - val_accuracy: 0.4811\n",
      "Epoch 116/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3023 - accuracy: 0.4836 - val_loss: 1.3217 - val_accuracy: 0.4734\n",
      "Epoch 117/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3057 - accuracy: 0.4816 - val_loss: 1.3011 - val_accuracy: 0.4815\n",
      "Epoch 118/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3046 - accuracy: 0.4822 - val_loss: 1.3113 - val_accuracy: 0.4812\n",
      "Epoch 119/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3030 - accuracy: 0.4833 - val_loss: 1.3167 - val_accuracy: 0.4752\n",
      "Epoch 120/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3046 - accuracy: 0.4843 - val_loss: 1.3340 - val_accuracy: 0.4692\n",
      "Epoch 121/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3059 - accuracy: 0.4824 - val_loss: 1.3052 - val_accuracy: 0.4795\n",
      "Epoch 122/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3037 - accuracy: 0.4836 - val_loss: 1.3068 - val_accuracy: 0.4778\n",
      "Epoch 123/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2999 - accuracy: 0.4877 - val_loss: 1.2972 - val_accuracy: 0.4855\n",
      "Epoch 124/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3042 - accuracy: 0.4809 - val_loss: 1.3061 - val_accuracy: 0.4765\n",
      "Epoch 125/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.3005 - accuracy: 0.4832 - val_loss: 1.3548 - val_accuracy: 0.4646\n",
      "Epoch 126/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3047 - accuracy: 0.4822 - val_loss: 1.3045 - val_accuracy: 0.4883\n",
      "Epoch 127/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3024 - accuracy: 0.4842 - val_loss: 1.3065 - val_accuracy: 0.4774\n",
      "Epoch 128/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.3017 - accuracy: 0.4832 - val_loss: 1.3057 - val_accuracy: 0.4795\n",
      "Epoch 129/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3004 - accuracy: 0.4834 - val_loss: 1.3034 - val_accuracy: 0.4783\n",
      "Epoch 130/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2990 - accuracy: 0.4824 - val_loss: 1.3021 - val_accuracy: 0.4852\n",
      "Epoch 131/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3016 - accuracy: 0.4834 - val_loss: 1.3228 - val_accuracy: 0.4698\n",
      "Epoch 132/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2975 - accuracy: 0.4872 - val_loss: 1.2954 - val_accuracy: 0.4857\n",
      "Epoch 133/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2984 - accuracy: 0.4856 - val_loss: 1.2976 - val_accuracy: 0.4888\n",
      "Epoch 134/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.3007 - accuracy: 0.4854 - val_loss: 1.3157 - val_accuracy: 0.4780\n",
      "Epoch 135/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2968 - accuracy: 0.4852 - val_loss: 1.2946 - val_accuracy: 0.4860\n",
      "Epoch 136/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.3040 - accuracy: 0.4804 - val_loss: 1.2962 - val_accuracy: 0.4892\n",
      "Epoch 137/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2951 - accuracy: 0.4856 - val_loss: 1.2975 - val_accuracy: 0.4897\n",
      "Epoch 138/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2967 - accuracy: 0.4841 - val_loss: 1.2938 - val_accuracy: 0.4838\n",
      "Epoch 139/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2986 - accuracy: 0.4857 - val_loss: 1.2954 - val_accuracy: 0.4863\n",
      "Epoch 140/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.2960 - accuracy: 0.4860 - val_loss: 1.3019 - val_accuracy: 0.4803\n",
      "Epoch 141/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.2992 - accuracy: 0.4842 - val_loss: 1.2993 - val_accuracy: 0.4818\n",
      "Epoch 142/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.2984 - accuracy: 0.4822 - val_loss: 1.2914 - val_accuracy: 0.4903\n",
      "Epoch 143/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.3039 - accuracy: 0.4832 - val_loss: 1.2954 - val_accuracy: 0.4877\n",
      "Epoch 144/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2984 - accuracy: 0.4867 - val_loss: 1.2940 - val_accuracy: 0.4866\n",
      "Epoch 145/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2938 - accuracy: 0.4868 - val_loss: 1.2933 - val_accuracy: 0.4866\n",
      "Epoch 146/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2945 - accuracy: 0.4867 - val_loss: 1.3028 - val_accuracy: 0.4862\n",
      "Epoch 147/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2959 - accuracy: 0.4854 - val_loss: 1.2918 - val_accuracy: 0.4898\n",
      "Epoch 148/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2967 - accuracy: 0.4840 - val_loss: 1.2992 - val_accuracy: 0.4894\n",
      "Epoch 149/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.2958 - accuracy: 0.4890 - val_loss: 1.3098 - val_accuracy: 0.4758\n",
      "Epoch 150/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.2943 - accuracy: 0.4890 - val_loss: 1.3021 - val_accuracy: 0.4774\n",
      "Epoch 151/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.2917 - accuracy: 0.4882 - val_loss: 1.3009 - val_accuracy: 0.4854\n",
      "Epoch 152/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2962 - accuracy: 0.4827 - val_loss: 1.2921 - val_accuracy: 0.4883\n",
      "Epoch 153/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.2923 - accuracy: 0.4855 - val_loss: 1.2990 - val_accuracy: 0.4855\n",
      "Epoch 154/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.2954 - accuracy: 0.4852 - val_loss: 1.2901 - val_accuracy: 0.4934\n",
      "Epoch 155/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2939 - accuracy: 0.4854 - val_loss: 1.2892 - val_accuracy: 0.4897\n",
      "Epoch 156/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2943 - accuracy: 0.4881 - val_loss: 1.2956 - val_accuracy: 0.4866\n",
      "Epoch 157/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2947 - accuracy: 0.4877 - val_loss: 1.2896 - val_accuracy: 0.4903\n",
      "Epoch 158/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2931 - accuracy: 0.4855 - val_loss: 1.3079 - val_accuracy: 0.4812\n",
      "Epoch 159/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2963 - accuracy: 0.4894 - val_loss: 1.2906 - val_accuracy: 0.4909\n",
      "Epoch 160/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2916 - accuracy: 0.4875 - val_loss: 1.2898 - val_accuracy: 0.4898\n",
      "Epoch 161/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2978 - accuracy: 0.4833 - val_loss: 1.3148 - val_accuracy: 0.4695\n",
      "Epoch 162/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2921 - accuracy: 0.4881 - val_loss: 1.2907 - val_accuracy: 0.4888\n",
      "Epoch 163/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2901 - accuracy: 0.4872 - val_loss: 1.2899 - val_accuracy: 0.4897\n",
      "Epoch 164/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2958 - accuracy: 0.4863 - val_loss: 1.3013 - val_accuracy: 0.4862\n",
      "Epoch 165/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.2956 - accuracy: 0.4855 - val_loss: 1.3167 - val_accuracy: 0.4778\n",
      "Epoch 166/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.2961 - accuracy: 0.4869 - val_loss: 1.2922 - val_accuracy: 0.4886\n",
      "Epoch 167/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2941 - accuracy: 0.4872 - val_loss: 1.2967 - val_accuracy: 0.4809\n",
      "Epoch 168/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.2906 - accuracy: 0.4871 - val_loss: 1.2886 - val_accuracy: 0.4871\n",
      "Epoch 169/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2902 - accuracy: 0.4859 - val_loss: 1.3153 - val_accuracy: 0.4765\n",
      "Epoch 170/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2897 - accuracy: 0.4861 - val_loss: 1.3988 - val_accuracy: 0.4520\n",
      "Epoch 171/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.2962 - accuracy: 0.4833 - val_loss: 1.2943 - val_accuracy: 0.4882\n",
      "Epoch 172/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.2927 - accuracy: 0.4897 - val_loss: 1.3307 - val_accuracy: 0.4722\n",
      "Epoch 173/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2891 - accuracy: 0.4885 - val_loss: 1.2852 - val_accuracy: 0.4918\n",
      "Epoch 174/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.2906 - accuracy: 0.4907 - val_loss: 1.2943 - val_accuracy: 0.4803\n",
      "Epoch 175/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.2917 - accuracy: 0.4885 - val_loss: 1.2853 - val_accuracy: 0.4968\n",
      "Epoch 176/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2901 - accuracy: 0.4868 - val_loss: 1.3013 - val_accuracy: 0.4792\n",
      "Epoch 177/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2902 - accuracy: 0.4894 - val_loss: 1.2868 - val_accuracy: 0.4883\n",
      "Epoch 178/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2916 - accuracy: 0.4877 - val_loss: 1.2854 - val_accuracy: 0.4918\n",
      "Epoch 179/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2920 - accuracy: 0.4869 - val_loss: 1.3052 - val_accuracy: 0.4815\n",
      "Epoch 180/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.2903 - accuracy: 0.4887 - val_loss: 1.2976 - val_accuracy: 0.4831\n",
      "Epoch 181/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.2921 - accuracy: 0.4844 - val_loss: 1.3074 - val_accuracy: 0.4849\n",
      "Epoch 182/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.2907 - accuracy: 0.4868 - val_loss: 1.2854 - val_accuracy: 0.4882\n",
      "Epoch 183/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.2885 - accuracy: 0.4890 - val_loss: 1.2868 - val_accuracy: 0.4895\n",
      "Epoch 184/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.2875 - accuracy: 0.4889 - val_loss: 1.2819 - val_accuracy: 0.4934\n",
      "Epoch 185/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.2875 - accuracy: 0.4903 - val_loss: 1.2906 - val_accuracy: 0.4882\n",
      "Epoch 186/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.2895 - accuracy: 0.4884 - val_loss: 1.2909 - val_accuracy: 0.4878\n",
      "Epoch 187/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.2891 - accuracy: 0.4873 - val_loss: 1.3164 - val_accuracy: 0.4808\n",
      "Epoch 188/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2877 - accuracy: 0.4888 - val_loss: 1.2968 - val_accuracy: 0.4802\n",
      "Epoch 189/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.2883 - accuracy: 0.4857 - val_loss: 1.2850 - val_accuracy: 0.4923\n",
      "Epoch 190/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2900 - accuracy: 0.4888 - val_loss: 1.2978 - val_accuracy: 0.4832\n",
      "Epoch 191/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.2873 - accuracy: 0.4903 - val_loss: 1.2866 - val_accuracy: 0.4852\n",
      "Epoch 192/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2866 - accuracy: 0.4911 - val_loss: 1.2898 - val_accuracy: 0.4854\n",
      "Epoch 193/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2884 - accuracy: 0.4897 - val_loss: 1.3400 - val_accuracy: 0.4729\n",
      "Epoch 194/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2910 - accuracy: 0.4863 - val_loss: 1.2886 - val_accuracy: 0.4895\n",
      "Epoch 195/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2872 - accuracy: 0.4902 - val_loss: 1.2805 - val_accuracy: 0.4952\n",
      "Epoch 196/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2879 - accuracy: 0.4889 - val_loss: 1.2860 - val_accuracy: 0.4918\n",
      "Epoch 197/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2885 - accuracy: 0.4900 - val_loss: 1.3042 - val_accuracy: 0.4772\n",
      "Epoch 198/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2870 - accuracy: 0.4899 - val_loss: 1.2827 - val_accuracy: 0.4902\n",
      "Epoch 199/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2886 - accuracy: 0.4898 - val_loss: 1.2861 - val_accuracy: 0.4923\n",
      "Epoch 200/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2841 - accuracy: 0.4938 - val_loss: 1.2835 - val_accuracy: 0.4840\n",
      "Epoch 201/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2905 - accuracy: 0.4893 - val_loss: 1.2853 - val_accuracy: 0.4891\n",
      "Epoch 202/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2844 - accuracy: 0.4895 - val_loss: 1.2823 - val_accuracy: 0.4949\n",
      "Epoch 203/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2867 - accuracy: 0.4884 - val_loss: 1.2906 - val_accuracy: 0.4849\n",
      "Epoch 204/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2844 - accuracy: 0.4897 - val_loss: 1.3027 - val_accuracy: 0.4778\n",
      "Epoch 205/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2866 - accuracy: 0.4883 - val_loss: 1.3015 - val_accuracy: 0.4849\n",
      "Epoch 206/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2879 - accuracy: 0.4907 - val_loss: 1.2837 - val_accuracy: 0.4874\n",
      "Epoch 207/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2894 - accuracy: 0.4866 - val_loss: 1.2838 - val_accuracy: 0.4929\n",
      "Epoch 208/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2808 - accuracy: 0.4894 - val_loss: 1.2836 - val_accuracy: 0.4902\n",
      "Epoch 209/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2869 - accuracy: 0.4907 - val_loss: 1.2820 - val_accuracy: 0.4891\n",
      "Epoch 210/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.2874 - accuracy: 0.4882 - val_loss: 1.3004 - val_accuracy: 0.4878\n",
      "Epoch 211/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2865 - accuracy: 0.4881 - val_loss: 1.2925 - val_accuracy: 0.4812\n",
      "Epoch 212/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2823 - accuracy: 0.4904 - val_loss: 1.2831 - val_accuracy: 0.4880\n",
      "Epoch 213/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2926 - accuracy: 0.4905 - val_loss: 1.2994 - val_accuracy: 0.4748\n",
      "Epoch 214/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2889 - accuracy: 0.4896 - val_loss: 1.2779 - val_accuracy: 0.4977\n",
      "Epoch 215/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2825 - accuracy: 0.4922 - val_loss: 1.2854 - val_accuracy: 0.4922\n",
      "Epoch 216/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2870 - accuracy: 0.4879 - val_loss: 1.3216 - val_accuracy: 0.4698\n",
      "Epoch 217/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2837 - accuracy: 0.4916 - val_loss: 1.2901 - val_accuracy: 0.4894\n",
      "Epoch 218/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2841 - accuracy: 0.4924 - val_loss: 1.2910 - val_accuracy: 0.4882\n",
      "Epoch 219/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2812 - accuracy: 0.4908 - val_loss: 1.2822 - val_accuracy: 0.4937\n",
      "Epoch 220/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2841 - accuracy: 0.4912 - val_loss: 1.2784 - val_accuracy: 0.4925\n",
      "Epoch 221/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2850 - accuracy: 0.4892 - val_loss: 1.2822 - val_accuracy: 0.4912\n",
      "Epoch 222/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2825 - accuracy: 0.4905 - val_loss: 1.3173 - val_accuracy: 0.4829\n",
      "Epoch 223/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2835 - accuracy: 0.4910 - val_loss: 1.2814 - val_accuracy: 0.4906\n",
      "Epoch 224/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2850 - accuracy: 0.4888 - val_loss: 1.2875 - val_accuracy: 0.4937\n",
      "Epoch 225/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2827 - accuracy: 0.4902 - val_loss: 1.2790 - val_accuracy: 0.4932\n",
      "Epoch 226/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2817 - accuracy: 0.4921 - val_loss: 1.2983 - val_accuracy: 0.4835\n",
      "Epoch 227/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2829 - accuracy: 0.4894 - val_loss: 1.2810 - val_accuracy: 0.4962\n",
      "Epoch 228/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2822 - accuracy: 0.4899 - val_loss: 1.2903 - val_accuracy: 0.4920\n",
      "Epoch 229/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2815 - accuracy: 0.4933 - val_loss: 1.2793 - val_accuracy: 0.4908\n",
      "Epoch 230/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2829 - accuracy: 0.4914 - val_loss: 1.2826 - val_accuracy: 0.4875\n",
      "Epoch 231/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2855 - accuracy: 0.4892 - val_loss: 1.2863 - val_accuracy: 0.4886\n",
      "Epoch 232/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.2808 - accuracy: 0.4898 - val_loss: 1.2992 - val_accuracy: 0.4795\n",
      "Epoch 233/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2858 - accuracy: 0.4919 - val_loss: 1.3054 - val_accuracy: 0.4791\n",
      "Epoch 234/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2769 - accuracy: 0.4931 - val_loss: 1.2974 - val_accuracy: 0.4797\n",
      "Epoch 235/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2833 - accuracy: 0.4872 - val_loss: 1.3027 - val_accuracy: 0.4777\n",
      "Epoch 236/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2798 - accuracy: 0.4892 - val_loss: 1.2772 - val_accuracy: 0.4922\n",
      "Epoch 237/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2885 - accuracy: 0.4872 - val_loss: 1.3132 - val_accuracy: 0.4726\n",
      "Epoch 238/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2833 - accuracy: 0.4909 - val_loss: 1.2773 - val_accuracy: 0.4937\n",
      "Epoch 239/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2827 - accuracy: 0.4906 - val_loss: 1.2942 - val_accuracy: 0.4854\n",
      "Epoch 240/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2816 - accuracy: 0.4910 - val_loss: 1.2834 - val_accuracy: 0.4920\n",
      "Epoch 241/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2856 - accuracy: 0.4909 - val_loss: 1.2749 - val_accuracy: 0.4932\n",
      "Epoch 242/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2785 - accuracy: 0.4927 - val_loss: 1.2858 - val_accuracy: 0.4920\n",
      "Epoch 243/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2826 - accuracy: 0.4915 - val_loss: 1.2937 - val_accuracy: 0.4825\n",
      "Epoch 244/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2790 - accuracy: 0.4911 - val_loss: 1.2820 - val_accuracy: 0.4923\n",
      "Epoch 245/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2809 - accuracy: 0.4894 - val_loss: 1.3085 - val_accuracy: 0.4800\n",
      "Epoch 246/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2839 - accuracy: 0.4902 - val_loss: 1.3007 - val_accuracy: 0.4872\n",
      "Epoch 247/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2810 - accuracy: 0.4898 - val_loss: 1.2809 - val_accuracy: 0.4932\n",
      "Epoch 248/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2810 - accuracy: 0.4902 - val_loss: 1.2876 - val_accuracy: 0.4929\n",
      "Epoch 249/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2847 - accuracy: 0.4916 - val_loss: 1.2787 - val_accuracy: 0.4937\n",
      "Epoch 250/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2812 - accuracy: 0.4907 - val_loss: 1.2770 - val_accuracy: 0.4903\n",
      "Epoch 251/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2805 - accuracy: 0.4914 - val_loss: 1.2858 - val_accuracy: 0.4943\n",
      "Epoch 252/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2792 - accuracy: 0.4923 - val_loss: 1.2875 - val_accuracy: 0.4871\n",
      "Epoch 253/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2787 - accuracy: 0.4920 - val_loss: 1.3022 - val_accuracy: 0.4808\n",
      "Epoch 254/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2807 - accuracy: 0.4946 - val_loss: 1.3080 - val_accuracy: 0.4746\n",
      "Epoch 255/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2836 - accuracy: 0.4943 - val_loss: 1.2823 - val_accuracy: 0.4928\n",
      "Epoch 256/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2803 - accuracy: 0.4938 - val_loss: 1.2908 - val_accuracy: 0.4875\n",
      "Epoch 257/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.2839 - accuracy: 0.4891 - val_loss: 1.3206 - val_accuracy: 0.4694\n",
      "Epoch 258/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2800 - accuracy: 0.4903 - val_loss: 1.2979 - val_accuracy: 0.4805\n",
      "Epoch 259/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2795 - accuracy: 0.4951 - val_loss: 1.2975 - val_accuracy: 0.4823\n",
      "Epoch 260/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.2784 - accuracy: 0.4900 - val_loss: 1.2781 - val_accuracy: 0.4992\n",
      "Epoch 261/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2769 - accuracy: 0.4918 - val_loss: 1.2764 - val_accuracy: 0.4948\n",
      "Epoch 262/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2806 - accuracy: 0.4910 - val_loss: 1.2893 - val_accuracy: 0.4915\n",
      "Epoch 263/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2790 - accuracy: 0.4904 - val_loss: 1.3093 - val_accuracy: 0.4762\n",
      "Epoch 264/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2795 - accuracy: 0.4910 - val_loss: 1.3086 - val_accuracy: 0.4758\n",
      "Epoch 265/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.2803 - accuracy: 0.4907 - val_loss: 1.2734 - val_accuracy: 0.4960\n",
      "Epoch 266/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2784 - accuracy: 0.4896 - val_loss: 1.3101 - val_accuracy: 0.4848\n",
      "Epoch 267/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2764 - accuracy: 0.4953 - val_loss: 1.3053 - val_accuracy: 0.4746\n",
      "Epoch 268/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2798 - accuracy: 0.4935 - val_loss: 1.2769 - val_accuracy: 0.4940\n",
      "Epoch 269/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.2784 - accuracy: 0.4913 - val_loss: 1.2766 - val_accuracy: 0.4969\n",
      "Epoch 270/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.2771 - accuracy: 0.4930 - val_loss: 1.2870 - val_accuracy: 0.4942\n",
      "Epoch 271/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2767 - accuracy: 0.4914 - val_loss: 1.2715 - val_accuracy: 0.4957\n",
      "Epoch 272/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.2790 - accuracy: 0.4913 - val_loss: 1.2766 - val_accuracy: 0.4948\n",
      "Epoch 273/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.2799 - accuracy: 0.4914 - val_loss: 1.2846 - val_accuracy: 0.4926\n",
      "Epoch 274/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2794 - accuracy: 0.4914 - val_loss: 1.2839 - val_accuracy: 0.4917\n",
      "Epoch 275/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2761 - accuracy: 0.4917 - val_loss: 1.2838 - val_accuracy: 0.4888\n",
      "Epoch 276/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.2800 - accuracy: 0.4910 - val_loss: 1.2814 - val_accuracy: 0.4932\n",
      "Epoch 277/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.2781 - accuracy: 0.4935 - val_loss: 1.2796 - val_accuracy: 0.4891\n",
      "Epoch 278/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2759 - accuracy: 0.4945 - val_loss: 1.2804 - val_accuracy: 0.4972\n",
      "Epoch 279/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2746 - accuracy: 0.4938 - val_loss: 1.2985 - val_accuracy: 0.4874\n",
      "Epoch 280/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.2804 - accuracy: 0.4916 - val_loss: 1.2758 - val_accuracy: 0.4962\n",
      "Epoch 281/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2775 - accuracy: 0.4927 - val_loss: 1.2759 - val_accuracy: 0.4889\n",
      "Epoch 282/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2763 - accuracy: 0.4965 - val_loss: 1.2741 - val_accuracy: 0.4974\n",
      "Epoch 283/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2791 - accuracy: 0.4910 - val_loss: 1.2769 - val_accuracy: 0.4886\n",
      "Epoch 284/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2772 - accuracy: 0.4937 - val_loss: 1.2862 - val_accuracy: 0.4865\n",
      "Epoch 285/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2749 - accuracy: 0.4924 - val_loss: 1.2754 - val_accuracy: 0.4989\n",
      "Epoch 286/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2826 - accuracy: 0.4898 - val_loss: 1.2873 - val_accuracy: 0.4877\n",
      "Epoch 287/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2776 - accuracy: 0.4934 - val_loss: 1.2751 - val_accuracy: 0.4986\n",
      "Epoch 288/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2760 - accuracy: 0.4912 - val_loss: 1.2764 - val_accuracy: 0.4922\n",
      "Epoch 289/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.2759 - accuracy: 0.4952 - val_loss: 1.2854 - val_accuracy: 0.4949\n",
      "Epoch 290/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2732 - accuracy: 0.4955 - val_loss: 1.2738 - val_accuracy: 0.4942\n",
      "Epoch 291/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2733 - accuracy: 0.4925 - val_loss: 1.3177 - val_accuracy: 0.4835\n",
      "Epoch 292/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2770 - accuracy: 0.4936 - val_loss: 1.3119 - val_accuracy: 0.4805\n",
      "Epoch 293/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2758 - accuracy: 0.4914 - val_loss: 1.2891 - val_accuracy: 0.4920\n",
      "Epoch 294/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2754 - accuracy: 0.4927 - val_loss: 1.2827 - val_accuracy: 0.4832\n",
      "Epoch 295/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2755 - accuracy: 0.4931 - val_loss: 1.2736 - val_accuracy: 0.4977\n",
      "Epoch 296/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2757 - accuracy: 0.4954 - val_loss: 1.2821 - val_accuracy: 0.4869\n",
      "Epoch 297/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2760 - accuracy: 0.4937 - val_loss: 1.2908 - val_accuracy: 0.4875\n",
      "Epoch 298/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2765 - accuracy: 0.4914 - val_loss: 1.2860 - val_accuracy: 0.4812\n",
      "Epoch 299/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2757 - accuracy: 0.4919 - val_loss: 1.2691 - val_accuracy: 0.4951\n",
      "Epoch 300/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2768 - accuracy: 0.4939 - val_loss: 1.2943 - val_accuracy: 0.4909\n",
      "Epoch 301/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2776 - accuracy: 0.4939 - val_loss: 1.2864 - val_accuracy: 0.4812\n",
      "Epoch 302/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2783 - accuracy: 0.4913 - val_loss: 1.2813 - val_accuracy: 0.4911\n",
      "Epoch 303/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2749 - accuracy: 0.4936 - val_loss: 1.2680 - val_accuracy: 0.4992\n",
      "Epoch 304/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2744 - accuracy: 0.4946 - val_loss: 1.2705 - val_accuracy: 0.4972\n",
      "Epoch 305/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2780 - accuracy: 0.4933 - val_loss: 1.2860 - val_accuracy: 0.4875\n",
      "Epoch 306/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2819 - accuracy: 0.4929 - val_loss: 1.2762 - val_accuracy: 0.4917\n",
      "Epoch 307/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2740 - accuracy: 0.4928 - val_loss: 1.2791 - val_accuracy: 0.4848\n",
      "Epoch 308/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2737 - accuracy: 0.4960 - val_loss: 1.2686 - val_accuracy: 0.4934\n",
      "Epoch 309/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2703 - accuracy: 0.4945 - val_loss: 1.3151 - val_accuracy: 0.4760\n",
      "Epoch 310/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2751 - accuracy: 0.4936 - val_loss: 1.2838 - val_accuracy: 0.4957\n",
      "Epoch 311/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2756 - accuracy: 0.4958 - val_loss: 1.2847 - val_accuracy: 0.4957\n",
      "Epoch 312/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2804 - accuracy: 0.4948 - val_loss: 1.2969 - val_accuracy: 0.4872\n",
      "Epoch 313/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2746 - accuracy: 0.4914 - val_loss: 1.2963 - val_accuracy: 0.4828\n",
      "Epoch 314/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2787 - accuracy: 0.4906 - val_loss: 1.2934 - val_accuracy: 0.4825\n",
      "Epoch 315/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2764 - accuracy: 0.4953 - val_loss: 1.2702 - val_accuracy: 0.4926\n",
      "Epoch 316/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2753 - accuracy: 0.4920 - val_loss: 1.2847 - val_accuracy: 0.4829\n",
      "Epoch 317/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2736 - accuracy: 0.4937 - val_loss: 1.2714 - val_accuracy: 0.4911\n",
      "Epoch 318/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2724 - accuracy: 0.4956 - val_loss: 1.2730 - val_accuracy: 0.4918\n",
      "Epoch 319/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2760 - accuracy: 0.4934 - val_loss: 1.2903 - val_accuracy: 0.4832\n",
      "Epoch 320/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2751 - accuracy: 0.4929 - val_loss: 1.3107 - val_accuracy: 0.4869\n",
      "Epoch 321/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2809 - accuracy: 0.4938 - val_loss: 1.2738 - val_accuracy: 0.4882\n",
      "Epoch 322/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2728 - accuracy: 0.4959 - val_loss: 1.2783 - val_accuracy: 0.4935\n",
      "Epoch 323/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2720 - accuracy: 0.4941 - val_loss: 1.2960 - val_accuracy: 0.4845\n",
      "Epoch 324/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2733 - accuracy: 0.4932 - val_loss: 1.3052 - val_accuracy: 0.4789\n",
      "Epoch 325/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2740 - accuracy: 0.4938 - val_loss: 1.2709 - val_accuracy: 0.4895\n",
      "Epoch 326/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2780 - accuracy: 0.4928 - val_loss: 1.2681 - val_accuracy: 0.4983\n",
      "Epoch 327/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2758 - accuracy: 0.4919 - val_loss: 1.2683 - val_accuracy: 0.4926\n",
      "Epoch 328/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2730 - accuracy: 0.4951 - val_loss: 1.2736 - val_accuracy: 0.4882\n",
      "Epoch 329/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2742 - accuracy: 0.4933 - val_loss: 1.2773 - val_accuracy: 0.4857\n",
      "Epoch 330/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2737 - accuracy: 0.4962 - val_loss: 1.2717 - val_accuracy: 0.4931\n",
      "Epoch 331/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2727 - accuracy: 0.4954 - val_loss: 1.2661 - val_accuracy: 0.4971\n",
      "Epoch 332/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2740 - accuracy: 0.4940 - val_loss: 1.2834 - val_accuracy: 0.4875\n",
      "Epoch 333/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2738 - accuracy: 0.4921 - val_loss: 1.3231 - val_accuracy: 0.4740\n",
      "Epoch 334/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2728 - accuracy: 0.4943 - val_loss: 1.3011 - val_accuracy: 0.4862\n",
      "Epoch 335/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2738 - accuracy: 0.4922 - val_loss: 1.2689 - val_accuracy: 0.4997\n",
      "Epoch 336/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2740 - accuracy: 0.4946 - val_loss: 1.2655 - val_accuracy: 0.4943\n",
      "Epoch 337/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2707 - accuracy: 0.4962 - val_loss: 1.2908 - val_accuracy: 0.4802\n",
      "Epoch 338/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.2753 - accuracy: 0.4930 - val_loss: 1.2760 - val_accuracy: 0.4925\n",
      "Epoch 339/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2719 - accuracy: 0.4931 - val_loss: 1.2855 - val_accuracy: 0.4834\n",
      "Epoch 340/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2765 - accuracy: 0.4921 - val_loss: 1.2673 - val_accuracy: 0.4989\n",
      "Epoch 341/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2761 - accuracy: 0.4937 - val_loss: 1.2709 - val_accuracy: 0.4965\n",
      "Epoch 342/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2736 - accuracy: 0.4917 - val_loss: 1.2651 - val_accuracy: 0.5005\n",
      "Epoch 343/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2709 - accuracy: 0.4915 - val_loss: 1.2916 - val_accuracy: 0.4917\n",
      "Epoch 344/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2726 - accuracy: 0.4969 - val_loss: 1.2733 - val_accuracy: 0.4978\n",
      "Epoch 345/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2741 - accuracy: 0.4948 - val_loss: 1.2906 - val_accuracy: 0.4811\n",
      "Epoch 346/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2750 - accuracy: 0.4969 - val_loss: 1.2762 - val_accuracy: 0.4954\n",
      "Epoch 347/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2739 - accuracy: 0.4955 - val_loss: 1.2791 - val_accuracy: 0.4880\n",
      "Epoch 348/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.2761 - accuracy: 0.4928 - val_loss: 1.3501 - val_accuracy: 0.4688\n",
      "Epoch 349/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2755 - accuracy: 0.4919 - val_loss: 1.2867 - val_accuracy: 0.4860\n",
      "Epoch 350/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.2740 - accuracy: 0.4982 - val_loss: 1.2829 - val_accuracy: 0.4938\n",
      "Epoch 351/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2715 - accuracy: 0.4932 - val_loss: 1.3584 - val_accuracy: 0.4622\n",
      "Epoch 352/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2735 - accuracy: 0.4957 - val_loss: 1.2752 - val_accuracy: 0.4943\n",
      "Epoch 353/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2739 - accuracy: 0.4912 - val_loss: 1.2852 - val_accuracy: 0.4877\n",
      "Epoch 354/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2727 - accuracy: 0.4950 - val_loss: 1.2709 - val_accuracy: 0.4985\n",
      "Epoch 355/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2700 - accuracy: 0.4982 - val_loss: 1.2807 - val_accuracy: 0.4872\n",
      "Epoch 356/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2752 - accuracy: 0.4926 - val_loss: 1.2666 - val_accuracy: 0.4974\n",
      "Epoch 357/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2753 - accuracy: 0.4952 - val_loss: 1.2736 - val_accuracy: 0.4898\n",
      "Epoch 358/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2759 - accuracy: 0.4964 - val_loss: 1.2737 - val_accuracy: 0.4865\n",
      "Epoch 359/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2745 - accuracy: 0.4923 - val_loss: 1.2722 - val_accuracy: 0.4915\n",
      "Epoch 360/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2705 - accuracy: 0.4949 - val_loss: 1.2648 - val_accuracy: 0.4968\n",
      "Epoch 361/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2688 - accuracy: 0.4978 - val_loss: 1.2794 - val_accuracy: 0.4852\n",
      "Epoch 362/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2792 - accuracy: 0.4921 - val_loss: 1.2713 - val_accuracy: 0.4892\n",
      "Epoch 363/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2722 - accuracy: 0.4944 - val_loss: 1.2875 - val_accuracy: 0.4929\n",
      "Epoch 364/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2720 - accuracy: 0.4969 - val_loss: 1.2839 - val_accuracy: 0.4922\n",
      "Epoch 365/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2769 - accuracy: 0.4937 - val_loss: 1.2962 - val_accuracy: 0.4820\n",
      "Epoch 366/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2710 - accuracy: 0.4974 - val_loss: 1.2733 - val_accuracy: 0.4920\n",
      "Epoch 367/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2736 - accuracy: 0.4944 - val_loss: 1.2737 - val_accuracy: 0.4918\n",
      "Epoch 368/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2762 - accuracy: 0.4931 - val_loss: 1.2685 - val_accuracy: 0.4998\n",
      "Epoch 369/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2698 - accuracy: 0.4954 - val_loss: 1.2842 - val_accuracy: 0.4923\n",
      "Epoch 370/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2731 - accuracy: 0.4952 - val_loss: 1.2762 - val_accuracy: 0.4948\n",
      "Epoch 371/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2733 - accuracy: 0.4973 - val_loss: 1.2733 - val_accuracy: 0.4995\n",
      "Epoch 372/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2761 - accuracy: 0.4906 - val_loss: 1.3096 - val_accuracy: 0.4817\n",
      "Epoch 373/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.2740 - accuracy: 0.4939 - val_loss: 1.2756 - val_accuracy: 0.4909\n",
      "Epoch 374/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2732 - accuracy: 0.4958 - val_loss: 1.2924 - val_accuracy: 0.4888\n",
      "Epoch 375/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2701 - accuracy: 0.4943 - val_loss: 1.2667 - val_accuracy: 0.4983\n",
      "Epoch 376/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2720 - accuracy: 0.4968 - val_loss: 1.2918 - val_accuracy: 0.4865\n",
      "Epoch 377/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2723 - accuracy: 0.4954 - val_loss: 1.2813 - val_accuracy: 0.4918\n",
      "Epoch 378/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2728 - accuracy: 0.4961 - val_loss: 1.2661 - val_accuracy: 0.5003\n",
      "Epoch 379/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2688 - accuracy: 0.4964 - val_loss: 1.2700 - val_accuracy: 0.4929\n",
      "Epoch 380/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2721 - accuracy: 0.4954 - val_loss: 1.2771 - val_accuracy: 0.4843\n",
      "Epoch 381/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2700 - accuracy: 0.4956 - val_loss: 1.2754 - val_accuracy: 0.4897\n",
      "Epoch 382/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2770 - accuracy: 0.4934 - val_loss: 1.2716 - val_accuracy: 0.4886\n",
      "Epoch 383/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2698 - accuracy: 0.4971 - val_loss: 1.3042 - val_accuracy: 0.4797\n",
      "Epoch 384/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2724 - accuracy: 0.4946 - val_loss: 1.2667 - val_accuracy: 0.4963\n",
      "Epoch 385/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2752 - accuracy: 0.4942 - val_loss: 1.2696 - val_accuracy: 0.4992\n",
      "Epoch 386/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2728 - accuracy: 0.4936 - val_loss: 1.2795 - val_accuracy: 0.4883\n",
      "Epoch 387/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2713 - accuracy: 0.4968 - val_loss: 1.2747 - val_accuracy: 0.4969\n",
      "Epoch 388/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2751 - accuracy: 0.4936 - val_loss: 1.3132 - val_accuracy: 0.4740\n",
      "Epoch 389/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2675 - accuracy: 0.4988 - val_loss: 1.2779 - val_accuracy: 0.4882\n",
      "Epoch 390/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2702 - accuracy: 0.4952 - val_loss: 1.2655 - val_accuracy: 0.4957\n",
      "Epoch 391/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2751 - accuracy: 0.4946 - val_loss: 1.2762 - val_accuracy: 0.4962\n",
      "Epoch 392/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2671 - accuracy: 0.4941 - val_loss: 1.2681 - val_accuracy: 0.4985\n",
      "Epoch 393/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2676 - accuracy: 0.4987 - val_loss: 1.2671 - val_accuracy: 0.4992\n",
      "Epoch 394/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2767 - accuracy: 0.4934 - val_loss: 1.2883 - val_accuracy: 0.4831\n",
      "Epoch 395/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2664 - accuracy: 0.4954 - val_loss: 1.2813 - val_accuracy: 0.4934\n",
      "Epoch 396/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2668 - accuracy: 0.5021 - val_loss: 1.3049 - val_accuracy: 0.4875\n",
      "Epoch 397/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2748 - accuracy: 0.4941 - val_loss: 1.2761 - val_accuracy: 0.4978\n",
      "Epoch 398/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2689 - accuracy: 0.4964 - val_loss: 1.2803 - val_accuracy: 0.5003\n",
      "Epoch 399/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2685 - accuracy: 0.4971 - val_loss: 1.2861 - val_accuracy: 0.4929\n",
      "Epoch 400/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2730 - accuracy: 0.4971 - val_loss: 1.2883 - val_accuracy: 0.4952\n",
      "Epoch 401/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2715 - accuracy: 0.4948 - val_loss: 1.2688 - val_accuracy: 0.5017\n",
      "Epoch 402/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2670 - accuracy: 0.4963 - val_loss: 1.2624 - val_accuracy: 0.5008\n",
      "Epoch 403/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2704 - accuracy: 0.4957 - val_loss: 1.2737 - val_accuracy: 0.4952\n",
      "Epoch 404/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2701 - accuracy: 0.4945 - val_loss: 1.2759 - val_accuracy: 0.4960\n",
      "Epoch 405/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2665 - accuracy: 0.4980 - val_loss: 1.2846 - val_accuracy: 0.4948\n",
      "Epoch 406/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2655 - accuracy: 0.4989 - val_loss: 1.2678 - val_accuracy: 0.4995\n",
      "Epoch 407/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2735 - accuracy: 0.4949 - val_loss: 1.2681 - val_accuracy: 0.4914\n",
      "Epoch 408/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2686 - accuracy: 0.4958 - val_loss: 1.2731 - val_accuracy: 0.4931\n",
      "Epoch 409/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2670 - accuracy: 0.4983 - val_loss: 1.2731 - val_accuracy: 0.4957\n",
      "Epoch 410/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2691 - accuracy: 0.4944 - val_loss: 1.2804 - val_accuracy: 0.4942\n",
      "Epoch 411/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2726 - accuracy: 0.4916 - val_loss: 1.2668 - val_accuracy: 0.4994\n",
      "Epoch 412/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2695 - accuracy: 0.4977 - val_loss: 1.2788 - val_accuracy: 0.4882\n",
      "Epoch 413/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2737 - accuracy: 0.4943 - val_loss: 1.2642 - val_accuracy: 0.4989\n",
      "Epoch 414/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2707 - accuracy: 0.4955 - val_loss: 1.2751 - val_accuracy: 0.4888\n",
      "Epoch 415/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2720 - accuracy: 0.4950 - val_loss: 1.2636 - val_accuracy: 0.4986\n",
      "Epoch 416/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2670 - accuracy: 0.4976 - val_loss: 1.2799 - val_accuracy: 0.4986\n",
      "Epoch 417/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2716 - accuracy: 0.4937 - val_loss: 1.2652 - val_accuracy: 0.4938\n",
      "Epoch 418/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2735 - accuracy: 0.4964 - val_loss: 1.2642 - val_accuracy: 0.4928\n",
      "Epoch 419/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2706 - accuracy: 0.4954 - val_loss: 1.2744 - val_accuracy: 0.4963\n",
      "Epoch 420/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2681 - accuracy: 0.4974 - val_loss: 1.2659 - val_accuracy: 0.5012\n",
      "Epoch 421/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2708 - accuracy: 0.4930 - val_loss: 1.2623 - val_accuracy: 0.5012\n",
      "Epoch 422/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2673 - accuracy: 0.4962 - val_loss: 1.2634 - val_accuracy: 0.4969\n",
      "Epoch 423/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2721 - accuracy: 0.4951 - val_loss: 1.2653 - val_accuracy: 0.4971\n",
      "Epoch 424/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2689 - accuracy: 0.4967 - val_loss: 1.2685 - val_accuracy: 0.4906\n",
      "Epoch 425/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2709 - accuracy: 0.4950 - val_loss: 1.2840 - val_accuracy: 0.4888\n",
      "Epoch 426/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2713 - accuracy: 0.4964 - val_loss: 1.2920 - val_accuracy: 0.4849\n",
      "Epoch 427/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2687 - accuracy: 0.4985 - val_loss: 1.3017 - val_accuracy: 0.4860\n",
      "Epoch 428/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2691 - accuracy: 0.4941 - val_loss: 1.2922 - val_accuracy: 0.4892\n",
      "Epoch 429/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2687 - accuracy: 0.4973 - val_loss: 1.2657 - val_accuracy: 0.5005\n",
      "Epoch 430/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2713 - accuracy: 0.4949 - val_loss: 1.2777 - val_accuracy: 0.4949\n",
      "Epoch 431/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2691 - accuracy: 0.4949 - val_loss: 1.2655 - val_accuracy: 0.4940\n",
      "Epoch 432/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2684 - accuracy: 0.4950 - val_loss: 1.2632 - val_accuracy: 0.4998\n",
      "Epoch 433/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2704 - accuracy: 0.4947 - val_loss: 1.2868 - val_accuracy: 0.4855\n",
      "Epoch 434/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2645 - accuracy: 0.4971 - val_loss: 1.2700 - val_accuracy: 0.4971\n",
      "Epoch 435/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2668 - accuracy: 0.4988 - val_loss: 1.2833 - val_accuracy: 0.4855\n",
      "Epoch 436/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2701 - accuracy: 0.4975 - val_loss: 1.2794 - val_accuracy: 0.4885\n",
      "Epoch 437/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2687 - accuracy: 0.4980 - val_loss: 1.2634 - val_accuracy: 0.4937\n",
      "Epoch 438/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2663 - accuracy: 0.4979 - val_loss: 1.2713 - val_accuracy: 0.4938\n",
      "Epoch 439/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2747 - accuracy: 0.4915 - val_loss: 1.2804 - val_accuracy: 0.4852\n",
      "Epoch 440/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.2656 - accuracy: 0.4981 - val_loss: 1.2627 - val_accuracy: 0.4937\n",
      "Epoch 441/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2687 - accuracy: 0.4988 - val_loss: 1.2653 - val_accuracy: 0.4949\n",
      "Epoch 442/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2701 - accuracy: 0.4967 - val_loss: 1.2741 - val_accuracy: 0.4886\n",
      "Epoch 443/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2681 - accuracy: 0.4957 - val_loss: 1.2855 - val_accuracy: 0.4868\n",
      "Epoch 444/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2696 - accuracy: 0.4956 - val_loss: 1.2667 - val_accuracy: 0.4923\n",
      "Epoch 445/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.2676 - accuracy: 0.4986 - val_loss: 1.2909 - val_accuracy: 0.4811\n",
      "Epoch 446/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2680 - accuracy: 0.4993 - val_loss: 1.2658 - val_accuracy: 0.4995\n",
      "Epoch 447/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2675 - accuracy: 0.4964 - val_loss: 1.2750 - val_accuracy: 0.4902\n",
      "Epoch 448/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2726 - accuracy: 0.4964 - val_loss: 1.2945 - val_accuracy: 0.4818\n",
      "Epoch 449/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2683 - accuracy: 0.4931 - val_loss: 1.3289 - val_accuracy: 0.4742\n",
      "Epoch 450/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2740 - accuracy: 0.4968 - val_loss: 1.2868 - val_accuracy: 0.4965\n",
      "Epoch 451/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2666 - accuracy: 0.4981 - val_loss: 1.2782 - val_accuracy: 0.4926\n",
      "Epoch 452/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2719 - accuracy: 0.4965 - val_loss: 1.2810 - val_accuracy: 0.4932\n",
      "Epoch 453/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2687 - accuracy: 0.4962 - val_loss: 1.2968 - val_accuracy: 0.4831\n",
      "Epoch 454/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2709 - accuracy: 0.4967 - val_loss: 1.2623 - val_accuracy: 0.4969\n",
      "Epoch 455/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2647 - accuracy: 0.4988 - val_loss: 1.2630 - val_accuracy: 0.4934\n",
      "Epoch 456/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2695 - accuracy: 0.4962 - val_loss: 1.2724 - val_accuracy: 0.4968\n",
      "Epoch 457/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2698 - accuracy: 0.4955 - val_loss: 1.2780 - val_accuracy: 0.4898\n",
      "Epoch 458/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2698 - accuracy: 0.4985 - val_loss: 1.2647 - val_accuracy: 0.4975\n",
      "Epoch 459/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2649 - accuracy: 0.4968 - val_loss: 1.3165 - val_accuracy: 0.4762\n",
      "Epoch 460/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2671 - accuracy: 0.4977 - val_loss: 1.2654 - val_accuracy: 0.4998\n",
      "Epoch 461/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2675 - accuracy: 0.4986 - val_loss: 1.2612 - val_accuracy: 0.4983\n",
      "Epoch 462/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2694 - accuracy: 0.4974 - val_loss: 1.3087 - val_accuracy: 0.4722\n",
      "Epoch 463/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2734 - accuracy: 0.4944 - val_loss: 1.3260 - val_accuracy: 0.4817\n",
      "Epoch 464/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2698 - accuracy: 0.4958 - val_loss: 1.2651 - val_accuracy: 0.4968\n",
      "Epoch 465/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2683 - accuracy: 0.4979 - val_loss: 1.2802 - val_accuracy: 0.4945\n",
      "Epoch 466/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2696 - accuracy: 0.4973 - val_loss: 1.2683 - val_accuracy: 0.5011\n",
      "Epoch 467/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2655 - accuracy: 0.4988 - val_loss: 1.2697 - val_accuracy: 0.4931\n",
      "Epoch 468/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2651 - accuracy: 0.4986 - val_loss: 1.3020 - val_accuracy: 0.4855\n",
      "Epoch 469/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2708 - accuracy: 0.4963 - val_loss: 1.2775 - val_accuracy: 0.4892\n",
      "Epoch 470/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.2651 - accuracy: 0.4995 - val_loss: 1.2650 - val_accuracy: 0.4955\n",
      "Epoch 471/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2663 - accuracy: 0.4986 - val_loss: 1.2767 - val_accuracy: 0.4978\n",
      "Epoch 472/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2649 - accuracy: 0.4947 - val_loss: 1.2614 - val_accuracy: 0.4977\n",
      "Epoch 473/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2667 - accuracy: 0.4972 - val_loss: 1.2734 - val_accuracy: 0.4940\n",
      "Epoch 474/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2704 - accuracy: 0.4969 - val_loss: 1.2761 - val_accuracy: 0.4912\n",
      "Epoch 475/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.2624 - accuracy: 0.5004 - val_loss: 1.2709 - val_accuracy: 0.4935\n",
      "Epoch 476/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2663 - accuracy: 0.4988 - val_loss: 1.2768 - val_accuracy: 0.4877\n",
      "Epoch 477/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2659 - accuracy: 0.4986 - val_loss: 1.2656 - val_accuracy: 0.4980\n",
      "Epoch 478/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2684 - accuracy: 0.4961 - val_loss: 1.3005 - val_accuracy: 0.4851\n",
      "Epoch 479/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2702 - accuracy: 0.4970 - val_loss: 1.2647 - val_accuracy: 0.5006\n",
      "Epoch 480/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2676 - accuracy: 0.4988 - val_loss: 1.2681 - val_accuracy: 0.4954\n",
      "Epoch 481/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2695 - accuracy: 0.4970 - val_loss: 1.2669 - val_accuracy: 0.4926\n",
      "Epoch 482/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.2655 - accuracy: 0.4997 - val_loss: 1.2618 - val_accuracy: 0.5008\n",
      "Epoch 483/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2673 - accuracy: 0.4979 - val_loss: 1.2661 - val_accuracy: 0.4972\n",
      "Epoch 484/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2649 - accuracy: 0.4965 - val_loss: 1.2664 - val_accuracy: 0.4900\n",
      "Epoch 485/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2651 - accuracy: 0.5004 - val_loss: 1.2605 - val_accuracy: 0.5028\n",
      "Epoch 486/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2651 - accuracy: 0.4979 - val_loss: 1.2636 - val_accuracy: 0.5035\n",
      "Epoch 487/500\n",
      "305/305 [==============================] - 0s 2ms/step - loss: 1.2689 - accuracy: 0.4980 - val_loss: 1.2728 - val_accuracy: 0.4898\n",
      "Epoch 488/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2673 - accuracy: 0.4977 - val_loss: 1.2661 - val_accuracy: 0.4978\n",
      "Epoch 489/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2697 - accuracy: 0.4947 - val_loss: 1.2633 - val_accuracy: 0.5005\n",
      "Epoch 490/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2705 - accuracy: 0.4963 - val_loss: 1.2668 - val_accuracy: 0.5000\n",
      "Epoch 491/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2661 - accuracy: 0.4971 - val_loss: 1.2681 - val_accuracy: 0.4972\n",
      "Epoch 492/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2636 - accuracy: 0.4973 - val_loss: 1.2681 - val_accuracy: 0.4915\n",
      "Epoch 493/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2685 - accuracy: 0.4977 - val_loss: 1.2640 - val_accuracy: 0.5035\n",
      "Epoch 494/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2669 - accuracy: 0.4963 - val_loss: 1.2657 - val_accuracy: 0.4994\n",
      "Epoch 495/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2638 - accuracy: 0.5010 - val_loss: 1.2617 - val_accuracy: 0.4983\n",
      "Epoch 496/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2653 - accuracy: 0.5010 - val_loss: 1.2947 - val_accuracy: 0.4906\n",
      "Epoch 497/500\n",
      "305/305 [==============================] - 1s 2ms/step - loss: 1.2651 - accuracy: 0.4971 - val_loss: 1.2608 - val_accuracy: 0.5031\n",
      "Epoch 498/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2666 - accuracy: 0.4982 - val_loss: 1.2704 - val_accuracy: 0.4975\n",
      "Epoch 499/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2676 - accuracy: 0.4954 - val_loss: 1.2687 - val_accuracy: 0.4971\n",
      "Epoch 500/500\n",
      "305/305 [==============================] - 0s 1ms/step - loss: 1.2650 - accuracy: 0.4979 - val_loss: 1.2668 - val_accuracy: 0.4966\n"
     ]
    }
   ],
   "source": [
    "logdir = \"./logs/pca\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "inputs1 = keras.Input(shape = 468)\n",
    "\n",
    "r = layers.Dense(24, activation=\"relu\")(inputs1)\n",
    "outputs1 = layers.Dense(6, activation=\"softmax\")(r)\n",
    "\n",
    "model1 = keras.Model(inputs1, outputs1)\n",
    "model1.summary()\n",
    "\n",
    "model1.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "callback = keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, # input\n",
    "    y_train, # output\n",
    "    batch_size=batch_size,\n",
    "    verbose=1, \n",
    "    epochs=500,\n",
    "    shuffle=True,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=[tensorboard_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "edd308e7c007ad85b362a562bfbc8a6438bc8604a7d190cd1423250b5323680b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('faces': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
